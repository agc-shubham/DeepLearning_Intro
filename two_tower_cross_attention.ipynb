{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Tower CNN with Cross-Attention Fusion for Wafer Pass/Fail Prediction\n",
    "\n",
    "This notebook demonstrates loading images from filepaths, training a Two-Tower CNN where the kernel image queries the input image features via cross-attention, and predicting pass/fail."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dataset class to load pairs of images and labels\n",
    "class WaferDataset(Dataset):\n",
    "    def __init__(self, kernel_paths, input_paths, labels, transform=None):\n",
    "        self.kernel_paths = kernel_paths\n",
    "        self.input_paths = input_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        kernel_img = Image.open(self.kernel_paths[idx]).convert('L')\n",
    "        input_img = Image.open(self.input_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            kernel_img = self.transform(kernel_img)\n",
    "            input_img = self.transform(input_img)\n",
    "        return kernel_img, input_img, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # grayscale mean/std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model Components\n",
    "Feature Extractor Tower, Cross-Attention Fusion, and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((8, 8))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)  # (B, 128, 8, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, feature_dim=128, spatial_dim=8):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.spatial_dim = spatial_dim\n",
    "        self.query_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_proj = nn.Linear(feature_dim, feature_dim)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "\n",
    "    def forward(self, feat_kernel, feat_input):\n",
    "        B, C, H, W = feat_kernel.shape\n",
    "        q = feat_kernel.permute(0, 2, 3, 1).reshape(B, H*W, C)  # Queries from kernel\n",
    "        k = feat_input.permute(0, 2, 3, 1).reshape(B, H*W, C)   # Keys from input\n",
    "        v = feat_input.permute(0, 2, 3, 1).reshape(B, H*W, C)   # Values from input\n",
    "        Q = self.query_proj(q)\n",
    "        K = self.key_proj(k)\n",
    "        V = self.value_proj(v)\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale  # (B, H*W, H*W)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.bmm(attn_weights, V)  # (B, H*W, C)\n",
    "        attn_output = attn_output.view(B, H, W, C).permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        pooled = F.adaptive_avg_pool2d(attn_output, (1, 1)).view(B, C)\n",
    "        return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TwoTowerCrossAttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kernel_tower = FeatureExtractor()\n",
    "        self.input_tower = FeatureExtractor()\n",
    "        self.attention = CrossAttentionFusion(feature_dim=128, spatial_dim=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, kernel_img, input_img):\n",
    "        feat_kernel = self.kernel_tower(kernel_img)\n",
    "        feat_input = self.input_tower(input_img)\n",
    "        fused = self.attention(feat_kernel, feat_input)\n",
    "        logits = self.classifier(fused)\n",
    "        return torch.sigmoid(logits).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loop with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    preds, targets = [], []\n",
    "    for k_img, i_img, label in dataloader:\n",
    "        k_img, i_img, label = k_img.to(device), i_img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(k_img, i_img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * label.size(0)\n",
    "        preds += (outputs > 0.5).long().cpu().tolist()\n",
    "        targets += label.long().cpu().tolist()\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(targets, preds)\n",
    "    epoch_f1 = f1_score(targets, preds)\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for k_img, i_img, label in dataloader:\n",
    "            k_img, i_img, label = k_img.to(device), i_img.to(device), label.to(device)\n",
    "            outputs = model(k_img, i_img)\n",
    "            loss = criterion(outputs, label)\n",
    "            running_loss += loss.item() * label.size(0)\n",
    "            preds += (outputs > 0.5).long().cpu().tolist()\n",
    "            targets += label.long().cpu().tolist()\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(targets, preds)\n",
    "    epoch_f1 = f1_score(targets, preds)\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train(model, train_loader, val_loader, epochs, optimizer, scheduler, criterion, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, val_f1 = eval_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"Train loss: {train_loss:.4f} Acc: {train_acc:.4f} F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
    "             )\n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "- Prepare lists of filepaths and labels (`kernel_paths`, `input_paths`, `labels`)\n",
    "- Split into training and validation\n",
    "- Create datasets and dataloaders\n",
    "- Instantiate model, optimizer, scheduler, criterion\n",
    "- Call `train()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example dummy data (replace with real filepaths and labels)\n",
    "kernel_paths = ['/path/to/kernel1.png', '/path/to/kernel2.png']\n",
    "input_paths = ['/path/to/input1.png', '/path/to/input2.png']\n",
    "labels = [1, 0]  # 1=pass, 0=fail\n",
    "\n",
    "# For demonstration, split data into train/val\n",
    "train_dataset = WaferDataset(kernel_paths, input_paths, labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size=2)  # just for demo\n",
    "\n",
    "# Initialize model\n",
    "model = TwoTowerCrossAttentionClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Train (reduce epochs for demo)\n",
    "# train(model, train_loader, val_loader, epochs=10, optimizer=optimizer, scheduler=scheduler, criterion=criterion, device=device)\n",
    "print(\"Ready to train â€” replace dummy data with your dataset and call train()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
